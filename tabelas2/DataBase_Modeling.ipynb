{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9bc84c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "diretorio_destino = '.'\n",
    "df = pd.read_excel('../ContratosPublicos2024.xlsx')\n",
    "df.head()\n",
    "dataframes_por_coluna = {}\n",
    "for nome_coluna in df.columns:\n",
    "    novo_df = df[[nome_coluna]].copy()\n",
    "    nome_arquivo_limpo = nome_coluna.replace(' ', '_').replace('/', '_')\n",
    "    caminho_completo = os.path.join(diretorio_destino, f'{nome_arquivo_limpo}.csv')\n",
    "    novo_df.to_csv(caminho_completo, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4cfd4144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lido 'adjudicante.csv' (21748 linhas) e coluna renomeada para 'Entidade_Completa'.\n",
      "Lido 'adjudicatarios.csv' (21748 linhas) e coluna renomeada para 'Entidade_Completa'.\n",
      "\n",
      "Total de linhas combinadas (com duplicatas): 43496\n",
      "Duplicatas removidas (baseado em NIF e Nome): 32091\n",
      "\n",
      "‚ú® SUCESSO! O arquivo final 'entidades.csv' foi criado com 11405 linhas √∫nicas.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# --- Configura√ß√µes ---\n",
    "ARQUIVO_ADJUDICANTES = 'adjudicante.csv'\n",
    "ARQUIVO_ADJUDICATARIOS = 'adjudicatarios.csv'\n",
    "ARQUIVO_SAIDA = 'entidades.csv'\n",
    "\n",
    "# Nomes das colunas combinadas em CADA arquivo (Ajuste se estes nomes n√£o estiverem corretos!)\n",
    "COLUNA_COMBINADA_ADJUDICANTE = 'adjudicante'\n",
    "COLUNA_COMBINADA_ADJUDICATARIO = 'adjudicatarios'\n",
    "\n",
    "COLUNAS_PARA_UNICIDADE = ['NIF', 'Nome'] \n",
    "NOME_COLUNA_UNIFICADA = 'Entidade_Completa' # Nome tempor√°rio para a coluna combinada ap√≥s o carregamento\n",
    "\n",
    "# --- 1. Carregar, Selecionar e Renomear as Colunas ---\n",
    "try:\n",
    "    # 1.1 Carregar APENAS a coluna relevante de cada arquivo\n",
    "    df_adjudicantes = pd.read_csv(ARQUIVO_ADJUDICANTES, \n",
    "                                  usecols=[COLUNA_COMBINADA_ADJUDICANTE])\n",
    "    \n",
    "    df_adjudicatarios = pd.read_csv(ARQUIVO_ADJUDICATARIOS, \n",
    "                                    usecols=[COLUNA_COMBINADA_ADJUDICATARIO])\n",
    "    \n",
    "    # 1.2 Renomear a coluna em cada DataFrame para um nome UNIFICADO\n",
    "    df_adjudicantes.rename(columns={COLUNA_COMBINADA_ADJUDICANTE: NOME_COLUNA_UNIFICADA}, \n",
    "                           inplace=True)\n",
    "    df_adjudicatarios.rename(columns={COLUNA_COMBINADA_ADJUDICATARIO: NOME_COLUNA_UNIFICADA}, \n",
    "                             inplace=True)\n",
    "    \n",
    "    print(f\"Lido '{ARQUIVO_ADJUDICANTES}' ({len(df_adjudicantes)} linhas) e coluna renomeada para '{NOME_COLUNA_UNIFICADA}'.\")\n",
    "    print(f\"Lido '{ARQUIVO_ADJUDICATARIOS}' ({len(df_adjudicatarios)} linhas) e coluna renomeada para '{NOME_COLUNA_UNIFICADA}'.\")\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"ERRO: N√£o foi poss√≠vel encontrar o arquivo: {e.filename}. Certifique-se de que os arquivos est√£o no mesmo diret√≥rio do script.\")\n",
    "    exit()\n",
    "except ValueError:\n",
    "    print(f\"ERRO: Verifique os nomes das colunas. As colunas de entrada ('{COLUNA_COMBINADA_ADJUDICANTE}' e '{COLUNA_COMBINADA_ADJUDICATARIO}') n√£o foram encontradas.\")\n",
    "    exit()\n",
    "\n",
    "# --- 2. Divis√£o da Coluna Unificada ---\n",
    "\n",
    "# Processar ambos os DataFrames (agora ambos t√™m a coluna NOME_COLUNA_UNIFICADA)\n",
    "for df in [df_adjudicantes, df_adjudicatarios]:\n",
    "    \n",
    "    # Padronizar a coluna combinada (remover espa√ßos extras e preencher vazios)\n",
    "    df[NOME_COLUNA_UNIFICADA] = df[NOME_COLUNA_UNIFICADA].fillna('').astype(str).str.strip()\n",
    "    \n",
    "    # Usar .str.split() para dividir a coluna pelo primeiro ' - ' encontrado\n",
    "    # n=1 garante que divide apenas na primeira ocorr√™ncia: \"505111667\" e \"Urbe - Consultores...\"\n",
    "    df[['NIF', 'Nome']] = df[NOME_COLUNA_UNIFICADA].str.split(' - ', n=1, expand=True)\n",
    "\n",
    "    # Limpar espa√ßos em branco adicionais que possam surgir ap√≥s a divis√£o\n",
    "    df['NIF'] = df['NIF'].str.strip()\n",
    "    df['Nome'] = df['Nome'].str.strip()\n",
    "    \n",
    "    # Remover a coluna combinada original\n",
    "    df.drop(columns=[NOME_COLUNA_UNIFICADA], inplace=True)\n",
    "\n",
    "\n",
    "# --- 3. Concatena√ß√£o e Limpeza ---\n",
    "\n",
    "# Concatenar (empilhar) os DataFrames\n",
    "df_entidades_com_duplicatas = pd.concat(\n",
    "    [df_adjudicantes, df_adjudicatarios], \n",
    "    ignore_index=True\n",
    ")\n",
    "\n",
    "linhas_totais = len(df_entidades_com_duplicatas)\n",
    "print(f\"\\nTotal de linhas combinadas (com duplicatas): {linhas_totais}\")\n",
    "\n",
    "# Tratar casos onde NIF/Nome pode ser NaN ap√≥s a divis√£o\n",
    "df_entidades_com_duplicatas['NIF'] = df_entidades_com_duplicatas['NIF'].fillna('')\n",
    "df_entidades_com_duplicatas['Nome'] = df_entidades_com_duplicatas['Nome'].fillna('')\n",
    "\n",
    "\n",
    "# --- 4. Garantir Unicidade (Remover Duplicatas) ---\n",
    "\n",
    "# Remover duplicatas com base no par (NIF, Nome)\n",
    "df_entidades_unicas = df_entidades_com_duplicatas.drop_duplicates(\n",
    "    subset=COLUNAS_PARA_UNICIDADE, \n",
    "    keep='first'\n",
    ")\n",
    "\n",
    "linhas_unicas_sem_id = len(df_entidades_unicas)\n",
    "duplicatas_removidas = linhas_totais - linhas_unicas_sem_id\n",
    "\n",
    "print(f\"Duplicatas removidas (baseado em NIF e Nome): {duplicatas_removidas}\")\n",
    "\n",
    "# --- 5. Gerar Coluna ID √önico e Reordenar ---\n",
    "\n",
    "df_entidades_unicas = df_entidades_unicas.reset_index(drop=True)\n",
    "df_entidades_unicas['id'] = df_entidades_unicas.index + 1\n",
    "\n",
    "# Selecionar e reordenar as colunas finais (id, NIF, Nome)\n",
    "df_entidades_final = df_entidades_unicas[['id', 'NIF', 'Nome']]\n",
    "\n",
    "# --- 6. Guardar o CSV de Sa√≠da ---\n",
    "df_entidades_final.to_csv(ARQUIVO_SAIDA, index=False)\n",
    "\n",
    "# --- Fim ---\n",
    "print(f\"\\n‚ú® SUCESSO! O arquivo final '{ARQUIVO_SAIDA}' foi criado com {len(df_entidades_final)} linhas √∫nicas.\")\n",
    "    # Us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c4c8322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Processando: tipoprocedimento.csv ---\n",
      "Linhas originais: 21748\n",
      "Linhas com nulos removidas: 0\n",
      "Linhas duplicadas removidas: 21737\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linhas finais: 11\n",
      "‚úÖ Ficheiro 'tipoprocedimento.csv' processado e salvo com a coluna 'ID'.\n",
      "\n",
      "--- Processando: fundamentacao.csv ---\n",
      "Linhas originais: 21748\n",
      "Linhas com nulos removidas: 137\n",
      "Linhas duplicadas removidas: 21520\n",
      "Linhas finais: 91\n",
      "‚úÖ Ficheiro 'fundamentacao.csv' processado e salvo com a coluna 'ID'.\n",
      "\n",
      "--- Processando: DescrAcordoQuadro.csv ---\n",
      "Linhas originais: 21748\n",
      "Linhas com nulos removidas: 18203\n",
      "Linhas duplicadas removidas: 3289\n",
      "Linhas finais: 256\n",
      "‚úÖ Ficheiro 'DescrAcordoQuadro.csv' processado e salvo com a coluna 'ID'.\n",
      "\n",
      "--- Processando: localExecucao.csv ---\n",
      "Linhas originais: 21748\n",
      "Linhas com nulos removidas: 0\n",
      "Linhas duplicadas removidas: 21138\n",
      "Linhas finais: 610\n",
      "‚úÖ Ficheiro 'localExecucao.csv' processado e salvo com a coluna 'ID'.\n",
      "\n",
      "--- Processando: tipoContrato.csv ---\n",
      "Linhas originais: 21748\n",
      "Linhas com nulos removidas: 0\n",
      "Linhas duplicadas removidas: 21729\n",
      "Linhas finais: 19\n",
      "‚úÖ Ficheiro 'tipoContrato.csv' processado e salvo com a coluna 'ID'.\n",
      "\n",
      "--- Processamento de todos os ficheiros conclu√≠do. ---\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Lista dos caminhos completos dos ficheiros a processar\n",
    "FICHEIROS_A_PROCESSAR = [\n",
    "    'tipoprocedimento.csv',\n",
    "    '/home/afonso/bioinf/Vscode/sql/projeto/dozero/tabelas2/fundamentacao.csv',\n",
    "    '/home/afonso/bioinf/Vscode/sql/projeto/dozero/tabelas2/DescrAcordoQuadro.csv',\n",
    "    '/home/afonso/bioinf/Vscode/sql/projeto/dozero/tabelas2/localExecucao.csv',\n",
    "    '/home/afonso/bioinf/Vscode/sql/projeto/dozero/tabelas2/tipoContrato.csv'\n",
    "]\n",
    "\n",
    "# Itera sobre cada ficheiro na lista\n",
    "for caminho_completo in FICHEIROS_A_PROCESSAR:\n",
    "    \n",
    "    # Extrai apenas o nome do ficheiro para mensagens\n",
    "    nome_ficheiro = os.path.basename(caminho_completo)\n",
    "    \n",
    "    print(f\"\\n--- Processando: {nome_ficheiro} ---\")\n",
    "\n",
    "    try:\n",
    "        # 1. Carregar o DataFrame\n",
    "        df = pd.read_csv(caminho_completo)\n",
    "        linhas_originais = len(df)\n",
    "        print(f\"Linhas originais: {linhas_originais}\")\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"‚ö†Ô∏è ERRO: Ficheiro n√£o encontrado no caminho: {caminho_completo}\")\n",
    "        continue # Passa para o pr√≥ximo ficheiro na lista\n",
    "    \n",
    "    # 2. Remover Linhas com Nulos (NULLs/NaNs)\n",
    "    \n",
    "    # Antes da remo√ß√£o\n",
    "    nulos_antes = df.isnull().any(axis=1).sum()\n",
    "    \n",
    "    # Remove qualquer linha onde pelo menos um valor seja nulo (NaN)\n",
    "    df.dropna(how='any', inplace=True)\n",
    "    \n",
    "    nulos_removidos = nulos_antes\n",
    "    \n",
    "    print(f\"Linhas com nulos removidas: {nulos_removidos}\")\n",
    "    \n",
    "    # 3. Remover Repetidos (Duplicatas)\n",
    "    \n",
    "    # Antes da remo√ß√£o\n",
    "    duplicatas_antes = df.duplicated().sum()\n",
    "    \n",
    "    # Remove linhas duplicadas (mant√©m a primeira ocorr√™ncia)\n",
    "    df.drop_duplicates(keep='first', inplace=True)\n",
    "    \n",
    "    duplicatas_removidas = duplicatas_antes\n",
    "    \n",
    "    print(f\"Linhas duplicadas removidas: {duplicatas_removidas}\")\n",
    "    \n",
    "    # 4. Criar a Coluna ID √önico\n",
    "    \n",
    "    # Resetar o √≠ndice para garantir uma sequ√™ncia limpa (de 0 a N-1)\n",
    "    df = df.reset_index(drop=True)\n",
    "    \n",
    "    # Criar a coluna 'ID' come√ßando em 1\n",
    "    df['ID'] = df.index + 1\n",
    "    \n",
    "    # 5. Reordenar as colunas para colocar o ID no in√≠cio\n",
    "    \n",
    "    colunas_novas = ['ID'] + [col for col in df.columns if col != 'ID']\n",
    "    df = df[colunas_novas]\n",
    "    \n",
    "    # 6. Salvar o DataFrame processado (sobrescrevendo o original, ou use um novo nome)\n",
    "    df.to_csv(caminho_completo, index=False)\n",
    "    \n",
    "    print(f\"Linhas finais: {len(df)}\")\n",
    "    print(f\"‚úÖ Ficheiro '{nome_ficheiro}' processado e salvo com a coluna 'ID'.\")\n",
    "\n",
    "print(\"\\n--- Processamento de todos os ficheiros conclu√≠do. ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45818436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Processando: localExecucao.csv ---\n",
      "Linhas originais: 610\n",
      "\n",
      "As colunas ['pais', 'distrito', 'concelho'] foram criadas.\n",
      "Coluna 'localExecucao' original foi removida.\n",
      "Linhas finais: 610\n",
      "‚úÖ Ficheiro 'localExecucao.csv' salvo com as novas colunas de localiza√ß√£o.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# --- Configura√ß√µes ---\n",
    "CAMINHO_FICHEIRO = '/home/afonso/bioinf/Vscode/sql/projeto/dozero/tabelas2/localExecucao.csv'\n",
    "COLUNA_COMBINADA = 'localExecucao'\n",
    "NOVAS_COLUNAS = ['pais', 'distrito', 'concelho']\n",
    "\n",
    "print(f\"--- Processando: {os.path.basename(CAMINHO_FICHEIRO)} ---\")\n",
    "\n",
    "try:\n",
    "    # 1. Carregar o DataFrame\n",
    "    df = pd.read_csv(CAMINHO_FICHEIRO)\n",
    "    linhas_originais = len(df)\n",
    "    print(f\"Linhas originais: {linhas_originais}\")\n",
    "    \n",
    "    if COLUNA_COMBINADA not in df.columns:\n",
    "        print(f\"‚ö†Ô∏è ERRO: A coluna '{COLUNA_COMBINADA}' n√£o foi encontrada no ficheiro. Verifique a escrita.\")\n",
    "        exit()\n",
    "        \n",
    "except FileNotFoundError:\n",
    "    print(f\"‚ö†Ô∏è ERRO: Ficheiro n√£o encontrado no caminho: {CAMINHO_FICHEIRO}\")\n",
    "    exit()\n",
    "\n",
    "# 2. Padronizar a coluna combinada\n",
    "# Tratar como string e remover espa√ßos antes de dividir\n",
    "df[COLUNA_COMBINADA] = df[COLUNA_COMBINADA].astype(str).str.strip()\n",
    "\n",
    "# 3. Dividir a coluna 'localExecucao' (O FIX EST√Å AQUI!)\n",
    "# Usamos n=2 para garantir que a divis√£o ocorra no m√°ximo 2 vezes,\n",
    "# resultando sempre em 3 colunas (pa√≠s, distrito, concelho), mesmo que haja mais v√≠rgulas no dado.\n",
    "df[NOVAS_COLUNAS] = df[COLUNA_COMBINADA].str.split(',', n=2, expand=True)\n",
    "\n",
    "# 4. Limpeza P√≥s-Divis√£o e Remo√ß√£o da Coluna Original\n",
    "\n",
    "# Aplicar .str.strip() para remover espa√ßos em branco nas novas colunas\n",
    "for col in NOVAS_COLUNAS:\n",
    "    # Tratar valores que possam ter ficado vazios ou NaN\n",
    "    df[col] = df[col].astype(str).str.strip().replace('nan', '') \n",
    "    \n",
    "# Remover a Coluna Combinada Original\n",
    "df.drop(columns=[COLUNA_COMBINADA], inplace=True)\n",
    "\n",
    "# 5. Salvar o DataFrame Atualizado\n",
    "df.to_csv(CAMINHO_FICHEIRO, index=False)\n",
    "\n",
    "print(f\"\\nAs colunas {NOVAS_COLUNAS} foram criadas.\")\n",
    "print(f\"Coluna '{COLUNA_COMBINADA}' original foi removida.\")\n",
    "print(f\"Linhas finais: {len(df)}\")\n",
    "print(f\"‚úÖ Ficheiro '{os.path.basename(CAMINHO_FICHEIRO)}' salvo com as novas colunas de localiza√ß√£o.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30901043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Processando: fundamentacao.csv ---\n",
      "Linhas originais: 91\n",
      "\n",
      "As colunas 'Artigo', 'Numero_n' e 'Alinea' foram criadas.\n",
      "Coluna 'fundamentacao' original foi removida.\n",
      "Linhas finais: 91\n",
      "‚úÖ Ficheiro 'fundamentacao.csv' salvo com as novas colunas de fundamenta√ß√£o legal.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:52: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<>:52: SyntaxWarning: invalid escape sequence '\\.'\n",
      "/tmp/ipykernel_27266/1607551723.py:52: SyntaxWarning: invalid escape sequence '\\.'\n",
      "  df['Artigo'] = df['Artigo'].str.replace('\\.¬∫', '', regex=True).fillna('').str.strip()\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "\n",
    "# --- Configura√ß√µes ---\n",
    "CAMINHO_FICHEIRO = '/home/afonso/bioinf/Vscode/sql/projeto/dozero/tabelas2/fundamentacao.csv'\n",
    "COLUNA_COMBINADA = 'fundamentacao'\n",
    "\n",
    "print(f\"--- Processando: {os.path.basename(CAMINHO_FICHEIRO)} ---\")\n",
    "\n",
    "try:\n",
    "    # 1. Carregar o DataFrame\n",
    "    df = pd.read_csv(CAMINHO_FICHEIRO)\n",
    "    linhas_originais = len(df)\n",
    "    print(f\"Linhas originais: {linhas_originais}\")\n",
    "    \n",
    "    if COLUNA_COMBINADA not in df.columns:\n",
    "        print(f\"‚ö†Ô∏è ERRO: A coluna '{COLUNA_COMBINADA}' n√£o foi encontrada no ficheiro. Verifique a escrita.\")\n",
    "        exit()\n",
    "        \n",
    "except FileNotFoundError:\n",
    "    print(f\"‚ö†Ô∏è ERRO: Ficheiro n√£o encontrado no caminho: {CAMINHO_FICHEIRO}\")\n",
    "    exit()\n",
    "\n",
    "# 2. Padronizar a coluna combinada e extrair dados\n",
    "df[COLUNA_COMBINADA] = df[COLUNA_COMBINADA].astype(str).str.strip()\n",
    "\n",
    "# Express√µes regulares para extrair os componentes:\n",
    "# Captura o n√∫mero do Artigo, independentemente do que vier antes ou depois.\n",
    "REGEX_ARTIGO = r'Artigo\\s*(\\d+\\.¬∫|\\d+)'\n",
    "\n",
    "# Captura o n√∫mero do par√°grafo, se existir (n.¬∫ X)\n",
    "REGEX_NUMERO = r'n\\.¬∫\\s*(\\d+)'\n",
    "\n",
    "# Captura a al√≠nea, se existir (al√≠nea X)\n",
    "REGEX_ALINEA = r'al√≠nea\\s*([a-z])'\n",
    "\n",
    "\n",
    "# 3. Criar as Novas Colunas usando .str.extract()\n",
    "# Extrai o Artigo:\n",
    "df['Artigo'] = df[COLUNA_COMBINADA].str.extract(REGEX_ARTIGO, flags=re.IGNORECASE)\n",
    "\n",
    "# Extrai o N√∫mero (n.¬∫):\n",
    "df['Numero_n'] = df[COLUNA_COMBINADA].str.extract(REGEX_NUMERO, flags=re.IGNORECASE)\n",
    "\n",
    "# Extrai a Al√≠nea:\n",
    "df['Alinea'] = df[COLUNA_COMBINADA].str.extract(REGEX_ALINEA, flags=re.IGNORECASE)\n",
    "\n",
    "\n",
    "# 4. Limpeza P√≥s-Extra√ß√£o\n",
    "# Limpar 'Artigo' para remover o '.¬∫' e garantir que todos sejam apenas n√∫meros ou string vazia.\n",
    "df['Artigo'] = df['Artigo'].str.replace('\\.¬∫', '', regex=True).fillna('').str.strip()\n",
    "df['Numero_n'] = df['Numero_n'].fillna('').str.strip()\n",
    "df['Alinea'] = df['Alinea'].fillna('').str.strip()\n",
    "\n",
    "# 5. Remover a Coluna Combinada Original\n",
    "df.drop(columns=[COLUNA_COMBINADA], inplace=True)\n",
    "\n",
    "# 6. Salvar o DataFrame Atualizado\n",
    "df.to_csv(CAMINHO_FICHEIRO, index=False)\n",
    "\n",
    "print(f\"\\nAs colunas 'Artigo', 'Numero_n' e 'Alinea' foram criadas.\")\n",
    "print(f\"Coluna '{COLUNA_COMBINADA}' original foi removida.\")\n",
    "print(f\"Linhas finais: {len(df)}\")\n",
    "print(f\"‚úÖ Ficheiro '{os.path.basename(CAMINHO_FICHEIRO)}' salvo com as novas colunas de fundamenta√ß√£o legal.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93b8bdd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Processando: cpv.csv ---\n",
      "Linhas originais: 21748\n",
      "Linhas duplicadas removidas: 19480\n",
      "\n",
      "As colunas ['Codigo_CPV', 'Descricao'] foram criadas.\n",
      "Coluna 'cpv' original foi removida.\n",
      "Linhas finais: 2268\n",
      "‚úÖ Ficheiro 'cpv.csv' salvo com as novas colunas de CPV.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# --- Configura√ß√µes ---\n",
    "# Substitua 'cpv.csv' pelo caminho completo se n√£o estiver no diret√≥rio de trabalho\n",
    "CAMINHO_FICHEIRO = 'cpv.csv' \n",
    "COLUNA_COMBINADA = 'cpv'\n",
    "NOVAS_COLUNAS = ['Codigo_CPV', 'Descricao']\n",
    "DELIMITADOR = ' - ' # H√≠fen com espa√ßos ao lado\n",
    "\n",
    "print(f\"--- Processando: {os.path.basename(CAMINHO_FICHEIRO)} ---\")\n",
    "\n",
    "try:\n",
    "    # 1. Carregar o DataFrame\n",
    "    df = pd.read_csv(CAMINHO_FICHEIRO)\n",
    "    linhas_originais = len(df)\n",
    "    print(f\"Linhas originais: {linhas_originais}\")\n",
    "    \n",
    "    if COLUNA_COMBINADA not in df.columns:\n",
    "        print(f\"‚ö†Ô∏è ERRO: A coluna '{COLUNA_COMBINADA}' n√£o foi encontrada no ficheiro. Verifique a escrita.\")\n",
    "        exit()\n",
    "        \n",
    "except FileNotFoundError:\n",
    "    print(f\"‚ö†Ô∏è ERRO: Ficheiro n√£o encontrado no caminho: {CAMINHO_FICHEIRO}\")\n",
    "    exit()\n",
    "\n",
    "# 2. Padronizar a coluna combinada\n",
    "# Tratar como string e remover espa√ßos antes de dividir\n",
    "df[COLUNA_COMBINADA] = df[COLUNA_COMBINADA].astype(str).str.strip()\n",
    "\n",
    "# 3. Dividir a coluna 'cpv'\n",
    "# Usamos n=1 para garantir que a divis√£o ocorra apenas na primeira ocorr√™ncia de ' - ',\n",
    "# resultando em 2 colunas: C√≥digo e Descri√ß√£o.\n",
    "df[NOVAS_COLUNAS] = df[COLUNA_COMBINADA].str.split(DELIMITADOR, n=1, expand=True)\n",
    "\n",
    "# 4. Remover Duplicatas (Com base no c√≥digo e na descri√ß√£o)\n",
    "# Os seus dados t√™m exemplos de duplicatas que devem ser removidas (90611000-3).\n",
    "df_antes_duplicatas = len(df)\n",
    "df.drop_duplicates(subset=NOVAS_COLUNAS, keep='first', inplace=True)\n",
    "duplicatas_removidas = df_antes_duplicatas - len(df)\n",
    "print(f\"Linhas duplicadas removidas: {duplicatas_removidas}\")\n",
    "\n",
    "\n",
    "# 5. Limpeza P√≥s-Divis√£o\n",
    "for col in NOVAS_COLUNAS:\n",
    "    # Aplicar .str.strip() para remover espa√ßos e tratar NaNs como string vazia\n",
    "    df[col] = df[col].astype(str).str.strip().replace('nan', '') \n",
    "    \n",
    "# 6. Adicionar a coluna ID (se ela n√£o existir, assumindo que foi o passo anterior)\n",
    "if 'ID' not in df.columns:\n",
    "    df = df.reset_index(drop=True)\n",
    "    df['ID'] = df.index + 1\n",
    "    colunas_finais = ['ID'] + NOVAS_COLUNAS\n",
    "    df = df[colunas_finais]\n",
    "\n",
    "\n",
    "# 7. Remover a Coluna Combinada Original\n",
    "#df.drop(columns=[COLUNA_COMBINADA], inplace=True)\n",
    "\n",
    "# 8. Salvar o DataFrame Atualizado\n",
    "df.to_csv(CAMINHO_FICHEIRO, index=False)\n",
    "\n",
    "print(f\"\\nAs colunas {NOVAS_COLUNAS} foram criadas.\")\n",
    "print(f\"Coluna '{COLUNA_COMBINADA}' original foi removida.\")\n",
    "print(f\"Linhas finais: {len(df)}\")\n",
    "print(f\"‚úÖ Ficheiro '{os.path.basename(CAMINHO_FICHEIRO)}' salvo com as novas colunas de CPV.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8fa6ae17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Processamento conclu√≠do: Remo√ß√£o focada na coluna 'Codigo_CPV'.\n",
      "   Arquivo de entrada (linhas): 2268\n",
      "   Arquivo de sa√≠da (linhas): 2127\n",
      "   ‚û°Ô∏è Total de linhas removidas: 141\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def manter_primeira_ocorrencia_na_segunda_coluna(nome_arquivo_entrada, nome_arquivo_saida):\n",
    "    \"\"\"\n",
    "    Remove linhas duplicadas focando EXCLUSIVAMENTE na segunda coluna (√≠ndice 1).\n",
    "    Mant√©m a primeira ocorr√™ncia do valor na 2¬™ coluna e descarta as subsequentes, \n",
    "    independentemente dos valores nas outras colunas.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 1. Carregar o arquivo CSV\n",
    "        df = pd.read_csv(nome_arquivo_entrada)\n",
    "\n",
    "        # 2. Identificar a segunda coluna (√≠ndice 1)\n",
    "        if len(df.columns) < 2:\n",
    "            print(\"‚ùå ERRO: O arquivo n√£o tem pelo menos duas colunas.\")\n",
    "            return\n",
    "\n",
    "        nome_segunda_coluna = df.columns[1]\n",
    "\n",
    "        # 3. Remover duplicatas APENAS na 2¬™ coluna\n",
    "        # O par√¢metro 'subset' instrui o pandas a ignorar todas as outras colunas.\n",
    "        # 'keep=\"first\"' garante a preserva√ß√£o da primeira apari√ß√£o.\n",
    "        df_limpo = df.drop_duplicates(\n",
    "            subset=[nome_segunda_coluna], \n",
    "            keep='first'\n",
    "        )\n",
    "        \n",
    "        # 4. Reajustar o √≠ndice (opcional)\n",
    "        df_limpo = df_limpo.reset_index(drop=True)\n",
    "\n",
    "        # 5. Salvar o DataFrame limpo em um novo arquivo CSV\n",
    "        df_limpo.to_csv(nome_arquivo_saida, index=False)\n",
    "\n",
    "        num_linhas_originais = len(df)\n",
    "        num_linhas_finais = len(df_limpo)\n",
    "        \n",
    "        print(f\"‚úÖ Processamento conclu√≠do: Remo√ß√£o focada na coluna '{nome_segunda_coluna}'.\")\n",
    "        print(f\"   Arquivo de entrada (linhas): {num_linhas_originais}\")\n",
    "        print(f\"   Arquivo de sa√≠da (linhas): {num_linhas_finais}\")\n",
    "        print(f\"   ‚û°Ô∏è Total de linhas removidas: {num_linhas_originais - num_linhas_finais}\")\n",
    "            \n",
    "    except FileNotFoundError:\n",
    "        print(f\"‚ùå ERRO: O arquivo '{nome_arquivo_entrada}' n√£o foi encontrado.\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Ocorreu um erro: {e}\")\n",
    "\n",
    "# --- Chamada da fun√ß√£o ---\n",
    "manter_primeira_ocorrencia_na_segunda_coluna('cpv.csv', 'cpv_unico_pela_coluna_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f85d2a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- An√°lise da Coluna: 'Descricao' (√çndice 1) ---\n",
      "\n",
      "‚úÖ N√£o foram encontrados valores repetidos na segunda coluna.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def mostrar_valores_repetidos_segunda_coluna(nome_arquivo):\n",
    "    \"\"\"\n",
    "    Carrega o CSV e exibe a contagem de todos os valores repetidos na segunda coluna.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 1. Carregar o arquivo CSV\n",
    "        df = pd.read_csv(nome_arquivo)\n",
    "\n",
    "        # 2. Identificar e selecionar a segunda coluna (√≠ndice 1)\n",
    "        if len(df.columns) < 2:\n",
    "            print(\"‚ùå ERRO: O arquivo tem menos de duas colunas.\")\n",
    "            return\n",
    "\n",
    "        nome_segunda_coluna = df.columns[2]\n",
    "        coluna = df[nome_segunda_coluna]\n",
    "        \n",
    "        print(f\"--- An√°lise da Coluna: '{nome_segunda_coluna}' (√çndice 1) ---\")\n",
    "\n",
    "        # 3. Identificar todas as ocorr√™ncias de valores duplicados\n",
    "        # duplicated(keep=False) marca TODAS as linhas onde o valor se repete.\n",
    "        duplicatas_series = coluna[coluna.duplicated(keep=False)]\n",
    "\n",
    "        if duplicatas_series.empty:\n",
    "            print(\"\\n‚úÖ N√£o foram encontrados valores repetidos na segunda coluna.\")\n",
    "        else:\n",
    "            # 4. Contar a frequ√™ncia de cada valor repetido\n",
    "            # O value_counts() lista os valores duplicados e a sua contagem.\n",
    "            frequencia_duplicatas = duplicatas_series.value_counts()\n",
    "            \n",
    "            print(f\"\\n‚ùå {len(frequencia_duplicatas)} VALORES DIFERENTES ENCONTRADOS REPETIDOS:\")\n",
    "            print(\"Abaixo est√° a lista desses valores e a frequ√™ncia (quantas vezes aparecem no total):\")\n",
    "            \n",
    "            # Exibir os resultados de forma clara\n",
    "            print(\"\\nValores Repetidos (2¬™ Coluna) | Contagem Total\")\n",
    "            print(\"-----------------------------------|----------------\")\n",
    "            # Loop para imprimir de forma formatada\n",
    "            for valor, contagem in frequencia_duplicatas.items():\n",
    "                print(f\"{valor:<35}| {contagem}\")\n",
    "            \n",
    "            print(f\"\\nTotal de linhas que cont√™m valores duplicados (todas as ocorr√™ncias): {len(duplicatas_series)}\")\n",
    "            print(f\"Total de valores √∫nicos na coluna: {coluna.nunique()}\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"\\n‚ùå ERRO: O arquivo '{nome_arquivo}' n√£o foi encontrado.\")\n",
    "        print(\"Por favor, garanta que o 'cpv.csv' est√° no mesmo diret√≥rio do seu script.\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Ocorreu um erro inesperado: {e}\")\n",
    "\n",
    "# --- Chamada da fun√ß√£o ---\n",
    "mostrar_valores_repetidos_segunda_coluna('cpv_unico_pela_coluna_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2db50314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üéâ DataFrame 'contratos' unido corretamente!\n",
      "   idcontrato  prazoExecucao  precoContratual dataCelebracaoContrato  \\\n",
      "0    10424261            366          3918.75             2024-01-01   \n",
      "1    10424271            366           467.95             2024-01-02   \n",
      "2    10424433             30         27849.00             2024-01-02   \n",
      "3    10424474            365         11520.00             2024-01-02   \n",
      "4    10424593            365         19248.00             2024-01-02   \n",
      "\n",
      "  dataPublicacao ProcedimentoCentralizado  \\\n",
      "0     2024-01-01                      N√£o   \n",
      "1     2024-01-02                      N√£o   \n",
      "2     2024-01-02                      N√£o   \n",
      "3     2024-01-02                      N√£o   \n",
      "4     2024-01-02                      N√£o   \n",
      "\n",
      "                                     objectoContrato  \n",
      "0  Seguro de Acidentes de Trabalho para os Funcio...  \n",
      "1          Seguro Autom√≥vel - FIAT 500L/1.3MJ LOUNGE  \n",
      "2              Liga√ß√£o EN 342 a placa de S. Martinho  \n",
      "3  Servi√ßos de limpeza de bermas, valetas, podas ...  \n",
      "4  Servi√ßos de limpeza de bermas, valetas e manut...  \n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Criacao contratos\n",
    "\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "\n",
    "nome_arquivo = 'contratos.csv'\n",
    "\n",
    "df1 = pd.read_csv(\"idcontrato.csv\")\n",
    "df2 = pd.read_csv(\"prazoExecucao.csv\")\n",
    "df3 = pd.read_csv(\"precoContratual.csv\")\n",
    "df4 = pd.read_csv(\"dataCelebracaoContrato.csv\")\n",
    "df5 = pd.read_csv(\"dataPublicacao.csv\")\n",
    "df6 = pd.read_csv(\"ProcedimentoCentralizado.csv\")\n",
    "df7 = pd.read_csv(\"objectoContrato.csv\")\n",
    "\n",
    "# 1. Corrija o nome da coluna chave em todos os DataFrames (VITAL)\n",
    "# Se ainda tiver d√∫vidas sobre o KeyError, execute este bloco:\n",
    "dataframes_para_concatenar = [df1, df2, df3, df4, df5, df6, df7]\n",
    "\n",
    "# O argumento 'axis=1' garante que os DataFrames s√£o unidos horizontalmente\n",
    "contratos = pd.concat(dataframes_para_concatenar, axis=1)\n",
    "print(\"\\nüéâ DataFrame 'contratos' unido corretamente!\")\n",
    "contratos.to_csv('contratos.csv',index=False,encoding='utf-8')\n",
    "print(contratos.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8656e92f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "## 2. Mapeamento da coluna 'tipoContrato'\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'tipoContrato'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/sql/lib/python3.12/site-packages/pandas/core/indexes/base.py:3812\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3811\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3812\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:167\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:196\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7096\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'tipoContrato'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 23\u001b[39m\n\u001b[32m     20\u001b[39m df_ref_tipo.rename(columns={\u001b[33m'\u001b[39m\u001b[33mID\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33mid_tipoContrato_num\u001b[39m\u001b[33m'\u001b[39m}, inplace=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# Assegurar que as colunas de jun√ß√£o t√™m o mesmo tipo de dado e est√£o limpas (VITAL)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m contratos[\u001b[33m'\u001b[39m\u001b[33mtipoContrato\u001b[39m\u001b[33m'\u001b[39m] = \u001b[43mcontratos\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtipoContrato\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m.astype(\u001b[38;5;28mstr\u001b[39m).str.strip()\n\u001b[32m     24\u001b[39m df_ref_tipo[\u001b[33m'\u001b[39m\u001b[33mtipoContrato\u001b[39m\u001b[33m'\u001b[39m] = df_ref_tipo[\u001b[33m'\u001b[39m\u001b[33mtipoContrato\u001b[39m\u001b[33m'\u001b[39m].astype(\u001b[38;5;28mstr\u001b[39m).str.strip()\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# 2. Jun√ß√£o (Merge) para adicionar o ID num√©rico\u001b[39;00m\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# Juntamos usando o nome por extenso como chave\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/sql/lib/python3.12/site-packages/pandas/core/frame.py:4113\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   4112\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m4113\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4114\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   4115\u001b[39m     indexer = [indexer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/sql/lib/python3.12/site-packages/pandas/core/indexes/base.py:3819\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3814\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   3815\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc.Iterable)\n\u001b[32m   3816\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[32m   3817\u001b[39m     ):\n\u001b[32m   3818\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[32m-> \u001b[39m\u001b[32m3819\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3820\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3821\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3822\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3823\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m   3824\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'tipoContrato'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "nome_arquivo_referencia = \"tipoContrato.csv\"\n",
    "nome_arquivo_saida = 'contratos_final_processado.csv'\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "## PASSO 2: MAPEAMENTO DO ID ('tipoContrato' por ID num√©rico)\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "print(\"\\n## 2. Mapeamento da coluna 'tipoContrato'\")\n",
    "\n",
    "# 1. Carregar o DataFrame de refer√™ncia\n",
    "try:\n",
    "    df_ref_tipo = pd.read_csv(nome_arquivo_referencia)\n",
    "    \n",
    "    # Prepara√ß√µes para a jun√ß√£o:\n",
    "    \n",
    "    # Renomear o ID na tabela de refer√™ncia (evita conflitos e prepara o nome final)\n",
    "    df_ref_tipo.rename(columns={'ID': 'id_tipoContrato_num'}, inplace=True)\n",
    "    \n",
    "    # Assegurar que as colunas de jun√ß√£o t√™m o mesmo tipo de dado e est√£o limpas (VITAL)\n",
    "    contratos['tipoContrato'] = contratos['tipoContrato'].astype(str).str.strip()\n",
    "    df_ref_tipo['tipoContrato'] = df_ref_tipo['tipoContrato'].astype(str).str.strip()\n",
    "\n",
    "    # 2. Jun√ß√£o (Merge) para adicionar o ID num√©rico\n",
    "    # Juntamos usando o nome por extenso como chave\n",
    "    contratos = contratos.merge(\n",
    "        df_ref_tipo[['tipoContrato', 'id_tipoContrato_num']],\n",
    "        on='tipoContrato',  \n",
    "        how='left'          \n",
    "    )\n",
    "\n",
    "    # 3. Limpeza Final:\n",
    "    # A) Remover a coluna antiga (nome por extenso)\n",
    "    contratos.drop(columns=['tipoContrato'], inplace=True)\n",
    "\n",
    "    # B) Renomear a nova coluna ID para o nome final\n",
    "    contratos.rename(columns={'id_tipoContrato_num': 'id_tipoContrato'}, inplace=True)\n",
    "\n",
    "    print(\"‚úÖ Mapeamento conclu√≠do: 'tipoContrato' substitu√≠do por 'id_tipoContrato'\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"‚ö†Ô∏è Aviso: Arquivo de refer√™ncia '{nome_arquivo_referencia}' n√£o encontrado. Mapeamento ignorado.\")\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "## PASSO 3: EXPORTA√á√ÉO PARA CSV\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "# Exporta o DataFrame 'contratos' (agora com o ID num√©rico) para o arquivo CSV\n",
    "contratos.to_csv(nome_arquivo_saida, index=False, encoding='utf-8')\n",
    "\n",
    "print(f\"\\n‚úÖ DataFrame final guardado com sucesso em: **{nome_arquivo_saida}**\")\n",
    "print(\"\\nPrimeiras linhas do resultado final:\")\n",
    "print(contratos.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sql",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
